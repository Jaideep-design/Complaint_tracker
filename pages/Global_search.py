# -*- coding: utf-8 -*-
"""
Streamlit app: Solar¬†AC Complaint Tracker
Added search filters for Customer¬†Name and Device¬†ID.
Created on Fri¬†Jul¬†¬†4¬†15:19:18¬†2025
@author:¬†Admin
"""
import re
import base64
import json
import streamlit as st
import pandas as pd
from datetime import datetime
import gspread
from google.oauth2 import service_account
from typing import Optional
from collections import Counter
from google.oauth2.service_account import Credentials

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SERVICE_ACCOUNT_FILE = r"C:\Users\Admin\Desktop\solar-ac-customer-mapping-905e295dd0db.json"

SHEET_ID_1 = "1px_3F6UEjE3hD6UQMoCGThI7X1o9AK4ERfqwfOKlfY4"
SHEET_ID_2 = "1z1dyhCXHLN3pSZBhRdpmJ3XwvwV9zF7_QJ0qsZRSLzU"
SHEET_ID_3 = "11CBVvoJjfgvAaFsS-3I_sqQxql8n53JfSZA8CGT9mvA"
SHEET_ID_4 = "1vBT1VxcajVkMQFqCQMCbmxGyggET3dX9RdrNWHvMu80"

COMMENTS_SHEET_ID = "1vqk13WA77LuSl0xzb54ESO6GSUfqiM9dUgdLfnWdaj0"
COMMENTS_SHEET_NAME = "solarac_Comments_log"

SELECTED_COLUMNS_1 = [
    "Ticket ID",
    "Name",
    "Master Controller Serial No.",
    "Inverter Serial No.",
    "Status",
    "Created At",
    "Problem",
    "Problem Description",
    "Mob No.",
    "Issue Resolutions Plan",
    "Site Address",
    'Serial Number',
    'Complaint Reg Date',
    'Resolution Method',
    'Component',
    'Problem',
    "Solution",
    "Remarks",
    "Part Type",
    "Part Description",
    'Total Breakdown',
    '1. AC Serial Number',
    'No of Solar panel',
    'Voltage',
    '1P-Voltage',
    'Battery Voltage',
    'Battery Capacity in AH',
    "Service Completion Date",
    "Service Completion Time",
]
SELECTED_COLUMNS_2 = [
    "Ticket ID",
    'Name',
    'Created At',
    'Mob No.',
    'Site Address',
    'Inverter Serial No.',
    'Issue Resolutions Plan',
    'Assigned Service Engineer',
    'Total Breakdown Time(in Days)'
    'Issue Resolutions Plan',
    'Ecozen-Master Controller Serial No.',
    "Remark",
    "Problem Description",
    "Date of Issue",
    'Status',
    'Additional Remark'
]
SELECTED_COLUMNS_3 = [
    "Phone Number",
    "Customer ID",
    "Customer Name",
    'Varient Name',
    'Remarks (if any)',
    "Part ID",
    "Part ID Description",
]
SELECTED_COLUMNS_4 = [
    "Date of Issue",
    "Created At",
    "Customer Name",
    "Master Controller Serial Number",
    "Ticket ID",
    "Problem Description",
    "Error Code",
    "Remark",
    "R&D Diagnostic Support Required"
]

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

RENAME_MAP_1 = {
    "Serial Number": "Serial Number (Used)",
    "Serial Number_2": "Serial Number (Returned)",
}

# Decode and load credentials from Streamlit Secrets
key_json = base64.b64decode(st.secrets["gcp_service_account"]["key_b64"]).decode("utf-8")
service_account_info = json.loads(key_json)

creds = service_account.Credentials.from_service_account_info(
    service_account_info, scopes=SCOPES
)

# creds = service_account.Credentials.from_service_account_file(
#     SERVICE_ACCOUNT_FILE, scopes=SCOPES
# )

gc = gspread.authorize(creds)
comment_ws = gc.open_by_key(COMMENTS_SHEET_ID).worksheet(COMMENTS_SHEET_NAME)
# %%
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ HELPERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def clean_phone_number(value):
    if pd.isna(value) or str(value).strip().upper() in ["NA", "", "#ERROR!"]:
        return None

    # If multiple numbers, take the first
    first_part = str(value).split(",")[0].strip()

    # Remove all non-digit characters
    digits = re.sub(r"\D", "", first_part)

    # Handle country code (e.g., 91 or +91)
    if len(digits) > 10:
        digits = digits[-10:]  # take last 10 digits
    
    return digits if len(digits) == 10 else None

def read_selected_columns(sheet_id, selected_columns, rename_duplicates=None):
    """
    Read selected columns from a Google Sheet, handling duplicate column names.
    Optionally rename specific duplicates via `rename_duplicates` dict.

    Args:
        sheet_id (str): Google Sheet ID
        selected_columns (list): Columns to select (base names)
        rename_duplicates (dict): Mapping like {"Serial Number_2": "Serial Number (Returned)"}
    """

    # Decode and load credentials from Streamlit Secrets
    key_json = base64.b64decode(st.secrets["gcp_service_account"]["key_b64"]).decode("utf-8")
    service_account_info = json.loads(key_json)

    # Authenticate using decoded credentials
    creds = service_account.Credentials.from_service_account_info(
        service_account_info,
        scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
    )
    
    # creds = service_account.Credentials.from_service_account_file(
    #     SERVICE_ACCOUNT_FILE, scopes=SCOPES
    # )
    
    gc = gspread.authorize(creds)

    # Get worksheet data
    worksheet = gc.open_by_key(sheet_id).sheet1
    data = worksheet.get_all_values()

    # Extract header and data
    raw_header = data[0]
    rows = data[1:]

    # Rename duplicate columns
    def make_unique(headers):
        counter = Counter()
        new_headers = []
        for h in headers:
            counter[h] += 1
            if counter[h] > 1:
                new_headers.append(f"{h}_{counter[h]}")
            else:
                new_headers.append(h)
        return new_headers

    header = make_unique(raw_header)

    # Apply to DataFrame
    df = pd.DataFrame(rows, columns=header)

    # Keep only selected columns based on base name
    def base_name(col):
        return col.split("_")[0] if "_" in col else col

    df = df[[col for col in df.columns if base_name(col) in selected_columns]]

    # Rename duplicates if mapping is provided
    if rename_duplicates:
        df.rename(columns=rename_duplicates, inplace=True)

    return df

def load_comments() -> pd.DataFrame:
    """Fetch the comments sheet as a tidy DataFrame (Topic, Timestamp,¬†Comment)."""

    try:
        records = comment_ws.get_all_records()
        df = pd.DataFrame(records)
        df.columns = df.columns.str.strip()
        if "Ticket ID" in df.columns and "Topic" not in df.columns:
            df = df.rename(columns={"Ticket ID": "Topic"})

        for col in ["Topic", "Timestamp", "Comment"]:
            if col not in df.columns:
                df[col] = None
        return df[["Topic", "Timestamp", "Comment"]]
    except Exception as e:  # noqa: BLE001
        st.error(f"Error reading comments: {e}")
        return pd.DataFrame(columns=["Topic", "Timestamp", "Comment"])

def process_sheets_and_transform() -> pd.DataFrame:
    """Read, merge, clean, and pivot data from the three Google Sheets."""

    # Step 1 ‚Äì Read sheets
    df1 = read_selected_columns(SHEET_ID_1, SELECTED_COLUMNS_1, rename_duplicates=RENAME_MAP_1)
    df2 = read_selected_columns(SHEET_ID_2, SELECTED_COLUMNS_2)
    
    df2 = df2[df2["Ticket ID"].notna() & (df2["Ticket ID"].astype(str).str.strip() != "")]
    
    df2["Mob No."] = df2["Mob No."].apply(clean_phone_number)
    df3 = read_selected_columns(SHEET_ID_3, SELECTED_COLUMNS_3)

    # Now safely filter
    df4 = read_selected_columns(SHEET_ID_4,SELECTED_COLUMNS_4)

    # Make duplicate column names unique by adding suffix _1, _2, etc.
    df4.columns = pd.Series(df4.columns).mask(
        df4.columns.duplicated(),
        df4.columns + '_2'
    )

    # Fill blanks (NaN or empty string) in Customer Name_2 with values from Customer Name
    df4["Customer Name_2"] = df4["Customer Name_2"].replace("", None)
    df4["Customer Name_2"] = df4["Customer Name_2"].fillna(df4["Customer Name"])

    # 1. Drop the original "Customer Name"
    df4 = df4.drop(columns=["Customer Name"])

    # 2. Rename Customer Name_2 ‚Üí Name
    rename_map = {
        "Customer Name_2": "Name",
        "Master Controller Serial Number": "Ecozen-Master Controller Serial No.",
        "Error Code": "Problem Description(if not mentioned)",
        "R&D Diagnostic Support Required": "RCA required"
    }

    df4 = df4.rename(columns=rename_map)
    
    # Append df_customer_care into df_customer_helpline
    df2 = pd.concat(
        [df2, df4],
        ignore_index=True
    )

    # Step 2 ‚Äì Merge Sheet 1 & 2 on "Ticket ID"
    df_merged = pd.merge(df1, df2, on="Ticket ID", how="outer")
    
    # Step ‚Äì Create unified 'Mob No.' column by taking first non-null value
    df_merged["Mob No."] = df_merged[["Mob No._x", "Mob No._y"]].bfill(axis=1).iloc[:, 0]
    
    # Normalize phone numbers: keep only last 10 digits of digits-only version
    df_merged["Mob No."] = (
        df_merged["Mob No."]
        .astype(str)
        .str.replace(r"\D", "", regex=True)  # Remove all non-digits
        .str[-10:]  # Keep only last 10 digits
    )

    # Step 3 ‚Äì Normalize phone numbers in df_merged and df3
    if "Mob No." in df_merged.columns:
        df_merged["Mob No."] = df_merged["Mob No."].str.replace(r"\D", "", regex=True).str[-10:]

    if "Phone Number" in df3.columns:
        df3["Phone Number"] = df3["Phone Number"].str.replace(r"\D", "", regex=True).str[-10:]
        
    # df_merged_filtered = df_merged[df_merged["Mob No."].notna()]

    # Filter df3 to only include non-empty, non-NaN phone numbers
    df3_filtered = df3[df3["Phone Number"].notna() & (df3["Phone Number"].str.strip() != "")]
    
    # Step 4 ‚Äì Merge df3 using phone numbers
    df_final = pd.merge(
        df_merged,
        df3_filtered,
        left_on="Mob No.",
        right_on="Phone Number",
        how="left"
    )

    # Drop unwanted columns before pivoting
    df_final = df_final.drop(columns=["Mob No._x", "Mob No._y", "Phone Number"], errors="ignore")

    # Step 5 ‚Äì Convert to vertical format
    vertical_rows = []
    for _, row in df_final.iterrows():
        ticket_id = row.get("Ticket ID")
        issue_date = row.get("Date of Issue")
        for col_name, val in row.items():
            if col_name in ["Ticket ID", "Date of Issue"]:
                continue
            if pd.isna(val) or str(val).strip() == "":
                continue
            vertical_rows.append(
                {
                    "Ticket ID": ticket_id,
                    "Issue_Date": issue_date,
                    "Fields": col_name,
                    "Value": val,
                }
            )

    vertical_df = pd.DataFrame(vertical_rows)
    vertical_df = vertical_df.sort_values(["Ticket ID", "Issue_Date", "Fields"])

    return vertical_df
# %%
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STREAMLIT UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(page_title="Solar¬†AC Complaint Tracker", layout="wide")
st.title("üìä Global search for Tickets")

# ---------- Refresh ----------
if st.button("üîÑ Refresh¬†&¬†Process Data"):
    with st.spinner("Fetching data from Google¬†Sheets‚Ä¶"):
        st.session_state.vertical_df = process_sheets_and_transform()
    st.success("‚úÖ Data refreshed and processed!")

# ---------- Main ----------
if "vertical_df" in st.session_state:
    vertical_df = st.session_state.vertical_df.copy()
    
    # ‚îÄ‚îÄ Sidebar filters ‚îÄ‚îÄ
    st.sidebar.header("üîé Filters")

    #---------------------------------------
    # Global search text input
    global_search = st.sidebar.text_input("üîç Global Search", "")

    # Initialize candidate tickets from all available ones
    candidate_ids = set(vertical_df["Ticket ID"].dropna())

    # Load comments (if not already loaded)
    comments_df = load_comments()

    # Define function to normalize text
    def normalize(text):
        if pd.isna(text):
            return ""
        return str(text).lower().replace(" ", "")

    if global_search:
        # Define keywords to identify relevant fields
        relevant_keywords = ["problem", "remark", "issue", "resolutions", "description", "observation", "plan", "name", "controller", "serial"]

        # Step 1: Identify relevant fields in vertical_df
        field_mask = vertical_df["Fields"].str.lower().apply(
            lambda x: any(keyword in x for keyword in relevant_keywords)
        )

        # Step 2: Normalize values and search term
        normalized_search = normalize(global_search)
        value_normalized = vertical_df["Value"].astype(str).apply(normalize)

        keyword_mask = field_mask & value_normalized.str.contains(normalized_search, na=False)

        # Step 3: Get ticket IDs from vertical_df
        ticket_ids_from_fields = set(vertical_df.loc[keyword_mask, "Ticket ID"])

        # Step 4: Check comments_df for matching text in comments
        if not comments_df.empty:
            comments_df["Normalized_Comment"] = comments_df["Comment"].astype(str).apply(normalize)
            comment_mask = comments_df["Normalized_Comment"].str.contains(normalized_search, na=False)
            ticket_ids_from_comments = set(comments_df.loc[comment_mask, "Topic"])
        else:
            ticket_ids_from_comments = set()

        # Step 5: Union both sources of ticket IDs and intersect with existing
        matched_ids = ticket_ids_from_fields | ticket_ids_from_comments
        candidate_ids &= matched_ids

    candidate_ids = sorted(candidate_ids)
    if not candidate_ids:
        st.sidebar.info("No tickets match the given search criteria.")
        st.stop()
    
    # ‚îÄ‚îÄ Matching Tickets Display ‚îÄ‚îÄ
    st.subheader(f"üéØ Tickets Matching '{global_search}'")
    
    # Filter vertical_df to include only rows from matching ticket IDs
    matching_df = vertical_df[vertical_df["Ticket ID"].isin(candidate_ids)].copy()
    
    # Function to extract first matching value for a field
    def get_field(ticket_slice, field_names):
        for fname in field_names:
            match = ticket_slice[ticket_slice["Fields"].str.strip().str.lower() == fname.strip().lower()]
            if not match.empty:
                return match.iloc[0]["Value"]
        return None
    
    # Build summary for matching tickets
    matching_display = []
    for ticket_id in matching_df["Ticket ID"].unique():
        ticket_slice = matching_df[matching_df["Ticket ID"] == ticket_id]
    
        device_no = get_field(ticket_slice, [
            "Master Controller Serial No.", "Ecozen-Master Controller Serial No."])
        problem_desc = get_field(ticket_slice, [
            "Problem description_x", "Problem description_y"])
        remark = get_field(ticket_slice, [
            "Remark", "Remarks"])
        issue_date = ticket_slice["Issue_Date"].iloc[0] if "Issue_Date" in ticket_slice.columns and not ticket_slice["Issue_Date"].isna().all() else None
    
        matching_display.append({
            "Ticket ID": ticket_id,
            "Issue Date": issue_date,
            "Device Number": device_no,
            "Problem Description": problem_desc,
            "Remark": remark
        })
    
    # Convert to DataFrame and show
    st.dataframe(pd.DataFrame(matching_display))

